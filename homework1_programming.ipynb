{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWXnuevqGZFa"
      },
      "source": [
        "# Homework 1 - Linear Regression\n",
        "\n",
        "In this assignment, you will implement linear regression from scratch using only numpy. You will also derive the derivatives yourself (i.e., no autograd using PyTorch, TensorFlow, etc.) and implement gradient descent for training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yx3_v8rGZFb"
      },
      "source": [
        "## Task 1: Inspect and Plot the Data\n",
        "\n",
        "First, let's generate some data and plot it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLB_y7EABey4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate some data\n",
        "np.random.seed(0)\n",
        "X = 2 * np.random.rand(20, 1)\n",
        "y = 4 + 3 * X + 2*np.random.randn(20, 1)\n",
        "\n",
        "# Plot the data\n",
        "plt.scatter(X, y)\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Generated Data')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVSRyiulGZFd"
      },
      "source": [
        "## Task 2: Implement Linear Regression from Scratch\n",
        "\n",
        "Linear regression models the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data. We will use gradient descent to train the model and derive the derivatives ourselves.\n",
        "\n",
        "### Step 1: Initialize Parameters\n",
        "\n",
        "Let's start by initializing the model parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeSMKZkMEtIm"
      },
      "outputs": [],
      "source": [
        "# Initialize parameters\n",
        "theta = np.random.randn(2, 1)\n",
        "learning_rate = 0.1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEdlQxg_GZFd"
      },
      "source": [
        "### Step 2: Compute the Prediction and Loss (1 pt)\n",
        "\n",
        "Implement the function to compute the predictions and the squared-error loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fG7Vs5gDFsFQ"
      },
      "outputs": [],
      "source": [
        "# Add x0 = 1 to each instance\n",
        "X_b = np.c_[np.ones((20, 1)), X]\n",
        "\n",
        "def predict(X, theta):\n",
        "    #TODO: Implement the prediction function\n",
        "\n",
        "\n",
        "def compute_loss(y, y_pred):\n",
        "    #TODO: Implement the loss function (remember to normalize by the size of the training set)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Mzoe0RGZFe"
      },
      "source": [
        "### Step 3: Compute the Gradient (0.5 pt)\n",
        "\n",
        "Compute the gradient of the loss function with respect to the parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYgAKeBYFsHX"
      },
      "outputs": [],
      "source": [
        "def compute_gradient(X, y, y_pred):\n",
        "    #TODO: Implement the gradient function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI2GHIHbGZFe"
      },
      "source": [
        "### Step 4: Train the Model using Gradient Descent (0.5 pt)\n",
        "\n",
        "Train the model using gradient descent and plot the loss over iterations. Come up with a stopping condition for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3DeLTKcFsJa"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(X, y, theta, learning_rate, num_iterations, tol=1e-6):\n",
        "    loss_history = []\n",
        "    for i in range(num_iterations):\n",
        "        y_pred = predict(X, theta)\n",
        "        loss = compute_loss(y, y_pred)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "        #TODO: Implement gradient descent\n",
        "\n",
        "    return theta, loss_history\n",
        "\n",
        "# Train the model\n",
        "num_iterations = 1000\n",
        "theta, loss_history = gradient_descent(X_b, y, theta, learning_rate, num_iterations)\n",
        "\n",
        "# Plot the loss over iterations\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss over Iterations')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inOybfkNGZFe"
      },
      "source": [
        "### Step 5: Plot the Regression Line (0.5 pt)\n",
        "\n",
        "Plot the regression line obtained from gradient descent along with the original data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUGclZTUEt4a"
      },
      "outputs": [],
      "source": [
        "# TODO: Create a plot of the regression line\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C376OkQFGZFf"
      },
      "source": [
        "## Task 3: Using L1 Loss\n",
        "\n",
        "Next, you will implement a regression model using L1 loss function instead of the squared-error (L2) loss function. This criterion is also known as [Least Absolute Deviation](https://https://en.wikipedia.org/wiki/Least_absolute_deviations) (LAD), in contrast with Minimum Squared Error (MSE) Objective. Models trained for LAD can be more robust and less sensitive to outliers.\n",
        "\n",
        "Implementing this will only require small changes to the model you just developed. Mostly, this will involve changing the loss function and its gradient (since the gradient computation for L1 is slightly tricky, the function for calculating the gradient has already been implemented for you). Train the model using gradient descent and compare the results.\n",
        "\n",
        "### Step 1: Implement L1 Loss Function (1 pt)\n",
        "\n",
        "Implement the L1 loss function and its gradient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAyo6CscEtZg"
      },
      "outputs": [],
      "source": [
        "def compute_l1_loss(y, y_pred):\n",
        "    #TODO: Implement the L1 loss function\n",
        "\n",
        "def compute_l1_gradient(X, y, y_pred):\n",
        "    return X.T.dot(np.sign(y_pred - y)) / len(y)\n",
        "\n",
        "def gradient_descent_l1(X, y, theta, learning_rate, num_iterations, tol=1e-6):\n",
        "    loss_history = []\n",
        "    for i in range(num_iterations):\n",
        "        y_pred = predict(X, theta)\n",
        "        loss = compute_l1_loss(y, y_pred)\n",
        "        loss_history.append(loss)\n",
        "        #TODO: Implement gradient steps\n",
        "\n",
        "    return theta, loss_history\n",
        "\n",
        "# Train the model using L1 loss\n",
        "theta = np.random.randn(2, 1)  # reinitialize parameters\n",
        "theta_l1, loss_history_l1 = gradient_descent_l1(X_b, y, theta, learning_rate, num_iterations)\n",
        "\n",
        "# Plot the L1 loss over iterations\n",
        "plt.plot(loss_history_l1)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('L1 Loss over Iterations')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro4CKs7UGZFf"
      },
      "source": [
        "### Step 2: Plot the Regression Line for L1 Loss (0.5 pt)\n",
        "\n",
        "Plot the regression line obtained from gradient descent using L1 loss along with the original data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2z_pqjjyEuQM"
      },
      "outputs": [],
      "source": [
        "# TODO: Create a plot of the regression line for the L1 loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc466ktsGZFf"
      },
      "source": [
        "Congratulations! You have successfully implemented linear regression from scratch using both squared-error loss and L1 loss, and trained the model using gradient descent."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
